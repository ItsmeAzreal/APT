# pretrain_ddp.py

"""
Distributed Streaming Pretraining Script (PyTorch DDP)
- Multi-GPU ready (torchrun --nproc_per_node=2 pretrain_ddp.py)
- Streaming + sharded dataset support (see Curriculum2BTokenDataset)
- AMP (mixed precision) for efficient GPU use
- TensorBoard logging (SummaryWriter)
- Clean structure, safe checkpointing, resume, val loss, and progress bar
"""

import os
import math
import argparse
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from torch.utils.tensorboard import SummaryWriter
from transformers import LlamaTokenizer, get_cosine_schedule_with_warmup
from tqdm import tqdm


import os
from dotenv import load_dotenv

load_dotenv(dotenv_path="APT/.env")

os.environ["HF_HOME"] = os.getenv("HF_HOME", "/default/path/hf_cache")
os.environ["TRANSFORMERS_CACHE"] = os.getenv("TRANSFORMERS_CACHE", "/default/path/hf_cache")
os.environ["HF_DATASETS_CACHE"] = os.getenv("HF_DATASETS_CACHE", "/default/path/hf_cache")




# -- Project-specific imports --
from src.config import *
from src.model import AnameeModel, count_parameters
from src.dataset import Curriculum2BTokenDataset, build_val_dataset
from src.validate import validate

# ------------------------- DDP Setup Functions -------------------------
def setup_ddp(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup_ddp():
    dist.destroy_process_group()

# ------------------------- Training Loop -------------------------
def train_loop_ddp(
    model, train_loader, optimizer, scheduler, device, scaler, start_step, total_steps,
    accumulation_steps, log_interval, writer, loss_spike_factor, rank,
    val_loader=None, val_interval=2000
):
    running_loss = 0
    tokens_seen = 0
    lowest_loss = None
    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()
    step = start_step

    model.train()
    pbar = tqdm(total=total_steps, initial=step, position=0, leave=True, dynamic_ncols=True, desc="Train Steps")

    while step < total_steps:
        for x, y in train_loader:
            x = x.to(device, non_blocking=True)
            y = y.to(device, non_blocking=True)
            bsz, seqlen = x.shape
            tokens_seen += bsz * seqlen

            try:
                if device.type == "cuda":
                    with torch.cuda.amp.autocast(dtype=torch.bfloat16 if use_bf16 else torch.float16):
                        logits = model(x)
                        loss = torch.nn.functional.cross_entropy(
                            logits.view(-1, logits.size(-1)), y.view(-1)
                        )
                    loss = loss / accumulation_steps
                    scaler.scale(loss).backward()
                else:
                    logits = model(x)
                    loss = torch.nn.functional.cross_entropy(
                        logits.view(-1, logits.size(-1)), y.view(-1)
                    )
                    loss = loss / accumulation_steps
                    loss.backward()
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"[Rank {rank}] OOM at step {step}, skipping batch")
                    torch.cuda.empty_cache()
                    continue
                else:
                    raise e

            if not math.isfinite(loss.item()):
                print(f"[Rank {rank}] Loss NaN/inf at step {step}!")
                break

            if lowest_loss is None or loss.item() < lowest_loss:
                lowest_loss = loss.item()
            if loss.item() > loss_spike_factor * lowest_loss:
                print(f"[Rank {rank}] Loss spike detected at step {step}! Stopping training.")
                break

            if (step + 1) % accumulation_steps == 0:
                if device.type == "cuda":
                    scaler.unscale_(optimizer)
                    grad = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad(set_to_none=True)
                else:
                    grad = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()
                    optimizer.zero_grad(set_to_none=True)
                scheduler.step()
                if writer is not None and rank == 0:
                    writer.add_scalar('grad_norm', grad, step + 1)

            running_loss += loss.item() * accumulation_steps

            if (step + 1) % log_interval == 0:
                avg_loss = running_loss / log_interval
                if rank == 0:
                    print(f"Step {step+1} | Train Loss: {avg_loss:.4f} | Tokens: {tokens_seen}")
                    if writer is not None:
                        writer.add_scalar('train_loss', avg_loss, step + 1)
                running_loss = 0

            # Validation
            if val_loader is not None and (step + 1) % val_interval == 0 and rank == 0:
                val_loss = validate(model, val_loader, device, val_steps=VAL_STEPS)
                print(f"Step {step+1} | Val Loss: {val_loss:.4f}")
                if writer is not None:
                    writer.add_scalar('val_loss', val_loss, step + 1)

            # Checkpointing
            if ((step + 1) % CHECKPOINT_INTERVAL == 0 or (step + 1) == total_steps) and rank == 0:
                os.makedirs(CHECKPOINT_DIR, exist_ok=True)
                torch.save({
                    "model": model.module.state_dict(),
                    "optimizer": optimizer.state_dict(),
                    "scheduler": scheduler.state_dict(),
                    "step": step + 1,
                    "tokens_seen": tokens_seen,
                }, os.path.join(CHECKPOINT_DIR, f"full_ckpt_step{step+1}.pt"))
                pbar.write(f"âœ… Checkpoint saved at step {step+1} ({tokens_seen} tokens)")
            step += 1
            pbar.update(1)
            if step >= total_steps:
                break
    pbar.close()

# ------------------------- Main Function -------------------------
def main(rank, world_size, args):
    setup_ddp(rank, world_size)
    device = torch.device(f"cuda:{rank}" if torch.cuda.is_available() else "cpu")

    tokenizer = LlamaTokenizer.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0')
    vocab_size = tokenizer.vocab_size

    # Streaming/sharding: each rank gets a shard
    train_dataset = Curriculum2BTokenDataset(
        tokenizer, BLOCK_SIZE, int(2_000_000_000 * 0.6), int(2_000_000_000 * 0.4),
        shard_id=rank, num_shards=world_size
    )

    sampler = DistributedSampler(
        train_dataset, num_replicas=world_size, rank=rank, shuffle=True
    )
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        sampler=sampler,
        num_workers=NUM_WORKERS,
        pin_memory=True,
        persistent_workers=False,
    )

    # --- Validation loader ---
    val_dataset = build_val_dataset(tokenizer, BLOCK_SIZE, val_size=VAL_STEPS)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)

    model = AnameeModel(
        vocab_size, MODEL_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS, NUM_KV_HEADS, BLOCK_SIZE
    ).to(device)
    model = DDP(model, device_ids=[rank])

    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.95))
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=WARMUP_STEPS,
        num_training_steps=TOTAL_TRAINING_STEPS
    )
    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None

    writer = SummaryWriter(RUNS_DIR) if rank == 0 else None

    # --- Resume logic ---
    start_step = 0
    tokens_seen = 0
    if RESUME_PATH and os.path.isfile(RESUME_PATH):
        print(f"Loading checkpoint from {RESUME_PATH}")
        ckpt = torch.load(RESUME_PATH, map_location=device)
        model.module.load_state_dict(ckpt["model"])
        optimizer.load_state_dict(ckpt["optimizer"])
        scheduler.load_state_dict(ckpt["scheduler"])
        start_step = ckpt.get("step", 0)
        tokens_seen = ckpt.get("tokens_seen", 0)
        print(f"Resumed from step {start_step} ({tokens_seen} tokens)")

    train_loop_ddp(
        model, train_loader, optimizer, scheduler, device, scaler,
        start_step, TOTAL_TRAINING_STEPS,
        ACCUMULATION_STEPS, LOG_INTERVAL, writer, LOSS_SPIKE_FACTOR, rank,
        val_loader=val_loader, val_interval=VAL_INTERVAL
    )

    if rank == 0:
        os.makedirs(CHECKPOINT_DIR, exist_ok=True)
        torch.save(model.module.state_dict(), os.path.join(CHECKPOINT_DIR, "final_model.pt"))
        print("Model saved at", os.path.join(CHECKPOINT_DIR, "final_model.pt"))

    cleanup_ddp()

# ------------------------- CLI & Distributed Launch -------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--world_size", type=int, default=int(os.environ.get("WORLD_SIZE", 2)),
                        help="Number of GPUs / distributed processes")
    args = parser.parse_args()
    world_size = args.world_size
    rank = int(os.environ["RANK"]) if "RANK" in os.environ else 0
    main(rank, world_size, args)
